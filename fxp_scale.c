#include <stdio.h>
#include <math.h>

#include "scale_inner.h"

// FFT Constants (SCALE = 26)
const fxc GM_TAB[1024] = {
FXC(0ull, 0ull),	FXC(0ull, 67108864ull),
FXC(47453133ull, 47453133ull),	FXC(-47453133ull, 47453133ull),
FXC(62000506ull, 25681450ull),	FXC(-25681450ull, 62000506ull),
FXC(25681450ull, 62000506ull),	FXC(-62000506ull, 25681450ull),
FXC(65819386ull, 13092290ull),	FXC(-13092290ull, 65819386ull),
FXC(37283687ull, 55798981ull),	FXC(-55798981ull, 37283687ull),
FXC(55798981ull, 37283687ull),	FXC(-37283687ull, 55798981ull),
FXC(13092290ull, 65819386ull),	FXC(-65819386ull, 13092290ull),
FXC(66785716ull, 6577819ull),	FXC(-6577819ull, 66785716ull),
FXC(42573413ull, 51875853ull),	FXC(-51875853ull, 42573413ull),
FXC(59184734ull, 31634900ull),	FXC(-31634900ull, 59184734ull),
FXC(19480675ull, 64219179ull),	FXC(-64219179ull, 19480675ull),
FXC(64219179ull, 19480675ull),	FXC(-19480675ull, 64219179ull),
FXC(31634900ull, 59184734ull),	FXC(-59184734ull, 31634900ull),
FXC(51875853ull, 42573413ull),	FXC(-42573413ull, 51875853ull),
FXC(6577819ull, 66785716ull),	FXC(-66785716ull, 6577819ull),
FXC(67028028ull, 3292876ull),	FXC(-3292876ull, 67028028ull),
FXC(45067559ull, 49724388ull),	FXC(-49724388ull, 45067559ull),
FXC(60665695ull, 28692737ull),	FXC(-28692737ull, 60665695ull),
FXC(22608295ull, 63185953ull),	FXC(-63185953ull, 22608295ull),
FXC(65097695ull, 16306124ull),	FXC(-16306124ull, 65097695ull),
FXC(34500851ull, 57561193ull),	FXC(-57561193ull, 34500851ull),
FXC(53902345ull, 39976704ull),	FXC(-39976704ull, 53902345ull),
FXC(9846915ull, 66382512ull),	FXC(-66382512ull, 9846915ull),
FXC(66382512ull, 9846915ull),	FXC(-9846915ull, 66382512ull),
FXC(39976704ull, 53902345ull),	FXC(-53902345ull, 39976704ull),
FXC(57561193ull, 34500851ull),	FXC(-34500851ull, 57561193ull),
FXC(16306124ull, 65097695ull),	FXC(-65097695ull, 16306124ull),
FXC(63185953ull, 22608295ull),	FXC(-22608295ull, 63185953ull),
FXC(28692737ull, 60665695ull),	FXC(-60665695ull, 28692737ull),
FXC(49724388ull, 45067559ull),	FXC(-45067559ull, 49724388ull),
FXC(3292876ull, 67028028ull),	FXC(-67028028ull, 3292876ull),
FXC(67088652ull, 1646934ull),	FXC(-1646934ull, 67088652ull),
FXC(46274283ull, 48603399ull),	FXC(-48603399ull, 46274283ull),
FXC(61351578ull, 27195284ull),	FXC(-27195284ull, 61351578ull),
FXC(24152147ull, 62612087ull),	FXC(-62612087ull, 24152147ull),
FXC(65478262ull, 14703635ull),	FXC(-14703635ull, 65478262ull),
FXC(35903083ull, 56697163ull),	FXC(-56697163ull, 35903083ull),
FXC(54867188ull, 38641834ull),	FXC(-38641834ull, 54867188ull),
FXC(11473058ull, 66120863ull),	FXC(-66120863ull, 11473058ull),
FXC(66604174ull, 8214841ull),	FXC(-8214841ull, 66604174ull),
FXC(41287493ull, 52905033ull),	FXC(-52905033ull, 41287493ull),
FXC(58390550ull, 33077838ull),	FXC(-33077838ull, 58390550ull),
FXC(17898790ull, 64677917ull),	FXC(-64677917ull, 17898790ull),
FXC(63721758ull, 21050825ull),	FXC(-21050825ull, 63721758ull),
FXC(30172906ull, 59943268ull),	FXC(-59943268ull, 30172906ull),
FXC(50815425ull, 43833687ull),	FXC(-43833687ull, 50815425ull),
FXC(4936834ull, 66927030ull),	FXC(-66927030ull, 4936834ull),
FXC(66927030ull, 4936834ull),	FXC(-4936834ull, 66927030ull),
FXC(43833687ull, 50815425ull),	FXC(-50815425ull, 43833687ull),
FXC(59943268ull, 30172906ull),	FXC(-30172906ull, 59943268ull),
FXC(21050825ull, 63721758ull),	FXC(-63721758ull, 21050825ull),
FXC(64677917ull, 17898790ull),	FXC(-17898790ull, 64677917ull),
FXC(33077838ull, 58390550ull),	FXC(-58390550ull, 33077838ull),
FXC(52905033ull, 41287493ull),	FXC(-41287493ull, 52905033ull),
FXC(8214841ull, 66604174ull),	FXC(-66604174ull, 8214841ull),
FXC(66120863ull, 11473058ull),	FXC(-11473058ull, 66120863ull),
FXC(38641834ull, 54867188ull),	FXC(-54867188ull, 38641834ull),
FXC(56697163ull, 35903083ull),	FXC(-35903083ull, 56697163ull),
FXC(14703635ull, 65478262ull),	FXC(-65478262ull, 14703635ull),
FXC(62612087ull, 24152147ull),	FXC(-24152147ull, 62612087ull),
FXC(27195284ull, 61351578ull),	FXC(-61351578ull, 27195284ull),
FXC(48603399ull, 46274283ull),	FXC(-46274283ull, 48603399ull),
FXC(1646934ull, 67088652ull),	FXC(-67088652ull, 1646934ull),
FXC(67103811ull, 823529ull),	FXC(-823529ull, 67103811ull),
FXC(46867237ull, 48031883ull),	FXC(-48031883ull, 46867237ull),
FXC(61680686ull, 26440358ull),	FXC(-26440358ull, 61680686ull),
FXC(24918675ull, 62310988ull),	FXC(-62310988ull, 24918675ull),
FXC(65653767ull, 13899009ull),	FXC(-13899009ull, 65653767ull),
FXC(36596140ull, 56252308ull),	FXC(-56252308ull, 36596140ull),
FXC(55337251ull, 37965619ull),	FXC(-37965619ull, 55337251ull),
FXC(12283599ull, 65975092ull),	FXC(-65975092ull, 12283599ull),
FXC(66699968ull, 7396887ull),	FXC(-7396887ull, 66699968ull),
FXC(41933610ull, 52394388ull),	FXC(-52394388ull, 41933610ull),
FXC(58792069ull, 32358805ull),	FXC(-32358805ull, 58792069ull),
FXC(18691140ull, 64453401ull),	FXC(-64453401ull, 18691140ull),
FXC(63975285ull, 20267276ull),	FXC(-20267276ull, 63975285ull),
FXC(30906230ull, 59568487ull),	FXC(-59568487ull, 30906230ull),
FXC(51349506ull, 43206803ull),	FXC(-43206803ull, 51349506ull),
FXC(5757760ull, 66861408ull),	FXC(-66861408ull, 5757760ull),
FXC(66982573ull, 4115165ull),	FXC(-4115165ull, 66982573ull),
FXC(44453970ull, 50273692ull),	FXC(-50273692ull, 44453970ull),
FXC(60309022ull, 29435038ull),	FXC(-29435038ull, 60309022ull),
FXC(21831204ull, 63458633ull),	FXC(-63458633ull, 21831204ull),
FXC(64892692ull, 17103745ull),	FXC(-17103745ull, 64892692ull),
FXC(33791889ull, 57980237ull),	FXC(-57980237ull, 33791889ull),
FXC(53407711ull, 40635158ull),	FXC(-40635158ull, 53407711ull),
FXC(9031558ull, 66498350ull),	FXC(-66498350ull, 9031558ull),
FXC(66256677ull, 10660790ull),	FXC(-10660790ull, 66256677ull),
FXC(39312229ull, 54388862ull),	FXC(-54388862ull, 39312229ull),
FXC(57133480ull, 35204618ull),	FXC(-35204618ull, 57133480ull),
FXC(15506047ull, 65292895ull),	FXC(-65292895ull, 15506047ull),
FXC(62903756ull, 23381982ull),	FXC(-23381982ull, 62903756ull),
FXC(27946115ull, 61013231ull),	FXC(-61013231ull, 27946115ull),
FXC(49167596ull, 45674360ull),	FXC(-45674360ull, 49167596ull),
FXC(2470091ull, 67063390ull),	FXC(-67063390ull, 2470091ull),
FXC(67063390ull, 2470091ull),	FXC(-2470091ull, 67063390ull),
FXC(45674360ull, 49167596ull),	FXC(-49167596ull, 45674360ull),
FXC(61013231ull, 27946115ull),	FXC(-27946115ull, 61013231ull),
FXC(23381982ull, 62903756ull),	FXC(-62903756ull, 23381982ull),
FXC(65292895ull, 15506047ull),	FXC(-15506047ull, 65292895ull),
FXC(35204618ull, 57133480ull),	FXC(-57133480ull, 35204618ull),
FXC(54388862ull, 39312229ull),	FXC(-39312229ull, 54388862ull),
FXC(10660790ull, 66256677ull),	FXC(-66256677ull, 10660790ull),
FXC(66498350ull, 9031558ull),	FXC(-9031558ull, 66498350ull),
FXC(40635158ull, 53407711ull),	FXC(-53407711ull, 40635158ull),
FXC(57980237ull, 33791889ull),	FXC(-33791889ull, 57980237ull),
FXC(17103745ull, 64892692ull),	FXC(-64892692ull, 17103745ull),
FXC(63458633ull, 21831204ull),	FXC(-21831204ull, 63458633ull),
FXC(29435038ull, 60309022ull),	FXC(-60309022ull, 29435038ull),
FXC(50273692ull, 44453970ull),	FXC(-44453970ull, 50273692ull),
FXC(4115165ull, 66982573ull),	FXC(-66982573ull, 4115165ull),
FXC(66861408ull, 5757760ull),	FXC(-5757760ull, 66861408ull),
FXC(43206803ull, 51349506ull),	FXC(-51349506ull, 43206803ull),
FXC(59568487ull, 30906230ull),	FXC(-30906230ull, 59568487ull),
FXC(20267276ull, 63975285ull),	FXC(-63975285ull, 20267276ull),
FXC(64453401ull, 18691140ull),	FXC(-18691140ull, 64453401ull),
FXC(32358805ull, 58792069ull),	FXC(-58792069ull, 32358805ull),
FXC(52394388ull, 41933610ull),	FXC(-41933610ull, 52394388ull),
FXC(7396887ull, 66699968ull),	FXC(-66699968ull, 7396887ull),
FXC(65975092ull, 12283599ull),	FXC(-12283599ull, 65975092ull),
FXC(37965619ull, 55337251ull),	FXC(-55337251ull, 37965619ull),
FXC(56252308ull, 36596140ull),	FXC(-36596140ull, 56252308ull),
FXC(13899009ull, 65653767ull),	FXC(-65653767ull, 13899009ull),
FXC(62310988ull, 24918675ull),	FXC(-24918675ull, 62310988ull),
FXC(26440358ull, 61680686ull),	FXC(-61680686ull, 26440358ull),
FXC(48031883ull, 46867237ull),	FXC(-46867237ull, 48031883ull),
FXC(823529ull, 67103811ull),	FXC(-67103811ull, 823529ull),
FXC(67107601ull, 411772ull),	FXC(-411772ull, 67107601ull),
FXC(47161073ull, 47743406ull),	FXC(-47743406ull, 47161073ull),
FXC(61841760ull, 26061395ull),	FXC(-26061395ull, 61841760ull),
FXC(25300539ull, 62156917ull),	FXC(-62156917ull, 25300539ull),
FXC(65737814ull, 13495904ull),	FXC(-13495904ull, 65737814ull),
FXC(36940609ull, 56026699ull),	FXC(-56026699ull, 36940609ull),
FXC(55569162ull, 37625361ull),	FXC(-37625361ull, 55569162ull),
FXC(12688183ull, 65898480ull),	FXC(-65898480ull, 12688183ull),
FXC(66744099ull, 6987485ull),	FXC(-6987485ull, 66744099ull),
FXC(42254307ull, 52136102ull),	FXC(-52136102ull, 42254307ull),
FXC(58989512ull, 31997455ull),	FXC(-31997455ull, 58989512ull),
FXC(19086267ull, 64337501ull),	FXC(-64337501ull, 19086267ull),
FXC(64098439ull, 19874350ull),	FXC(-19874350ull, 64098439ull),
FXC(31271153ull, 59377728ull),	FXC(-59377728ull, 31271153ull),
FXC(51613651ull, 42890915ull),	FXC(-42890915ull, 51613651ull),
FXC(6167906ull, 66824820ull),	FXC(-66824820ull, 6167906ull),
FXC(67006562ull, 3704090ull),	FXC(-3704090ull, 67006562ull),
FXC(44761607ull, 49999982ull),	FXC(-49999982ull, 44761607ull),
FXC(60488497ull, 29064434ull),	FXC(-29064434ull, 60488497ull),
FXC(22220168ull, 63323485ull),	FXC(-63323485ull, 22220168ull),
FXC(64996418ull, 16705249ull),	FXC(-16705249ull, 64996418ull),
FXC(34147013ull, 57771802ull),	FXC(-57771802ull, 34147013ull),
FXC(53656038ull, 40306690ull),	FXC(-40306690ull, 53656038ull),
FXC(9439415ull, 66441682ull),	FXC(-66441682ull, 9439415ull),
FXC(66320843ull, 10254046ull),	FXC(-10254046ull, 66320843ull),
FXC(39645212ull, 54146623ull),	FXC(-54146623ull, 39645212ull),
FXC(57348416ull, 34853391ull),	FXC(-34853391ull, 57348416ull),
FXC(15906385ull, 65196522ull),	FXC(-65196522ull, 15906385ull),
FXC(63046041ull, 22995571ull),	FXC(-22995571ull, 63046041ull),
FXC(28319959ull, 60840608ull),	FXC(-60840608ull, 28319959ull),
FXC(49446923ull, 45371813ull),	FXC(-45371813ull, 49446923ull),
FXC(2881538ull, 67046971ull),	FXC(-67046971ull, 2881538ull),
FXC(67077284ull, 2058551ull),	FXC(-2058551ull, 67077284ull),
FXC(45975187ull, 48886418ull),	FXC(-48886418ull, 45975187ull),
FXC(61183556ull, 27571218ull),	FXC(-27571218ull, 61183556ull),
FXC(23767512ull, 62759103ull),	FXC(-62759103ull, 23767512ull),
FXC(65386809ull, 15105126ull),	FXC(-15105126ull, 65386809ull),
FXC(35554519ull, 56916393ull),	FXC(-56916393ull, 35554519ull),
FXC(54629053ull, 38977765ull),	FXC(-38977765ull, 54629053ull),
FXC(11067132ull, 66190016ull),	FXC(-66190016ull, 11067132ull),
FXC(66552515ull, 8623362ull),	FXC(-8623362ull, 66552515ull),
FXC(40962097ull, 53157373ull),	FXC(-53157373ull, 40962097ull),
FXC(58186489ull, 33435493ull),	FXC(-33435493ull, 58186489ull),
FXC(17501597ull, 64786524ull),	FXC(-64786524ull, 17501597ull),
FXC(63591393ull, 21441418ull),	FXC(-21441418ull, 63591393ull),
FXC(29804533ull, 60127277ull),	FXC(-60127277ull, 29804533ull),
FXC(50545510ull, 44144660ull),	FXC(-44144660ull, 50545510ull),
FXC(4526085ull, 66956062ull),	FXC(-66956062ull, 4526085ull),
FXC(66895478ull, 5347398ull),	FXC(-5347398ull, 66895478ull),
FXC(43521065ull, 51083427ull),	FXC(-51083427ull, 43521065ull),
FXC(59757002ull, 30540143ull),	FXC(-30540143ull, 59757002ull),
FXC(20659440ull, 63849723ull),	FXC(-63849723ull, 20659440ull),
FXC(64566874ull, 18295309ull),	FXC(-18295309ull, 64566874ull),
FXC(32718937ull, 58592412ull),	FXC(-58592412ull, 32718937ull),
FXC(52650702ull, 41611335ull),	FXC(-41611335ull, 52650702ull),
FXC(7806011ull, 66653326ull),	FXC(-66653326ull, 7806011ull),
FXC(66049221ull, 11878552ull),	FXC(-11878552ull, 66049221ull),
FXC(38304447ull, 55103257ull),	FXC(-55103257ull, 38304447ull),
FXC(56475799ull, 36250294ull),	FXC(-36250294ull, 56475799ull),
FXC(14301591ull, 65567249ull),	FXC(-65567249ull, 14301591ull),
FXC(62462713ull, 24535873ull),	FXC(-24535873ull, 62462713ull),
FXC(26818326ull, 61517290ull),	FXC(-61517290ull, 26818326ull),
FXC(48318550ull, 46571636ull),	FXC(-46571636ull, 48318550ull),
FXC(1235255ull, 67097495ull),	FXC(-67097495ull, 1235255ull),
FXC(67097495ull, 1235255ull),	FXC(-1235255ull, 67097495ull),
FXC(46571636ull, 48318550ull),	FXC(-48318550ull, 46571636ull),
FXC(61517290ull, 26818326ull),	FXC(-26818326ull, 61517290ull),
FXC(24535873ull, 62462713ull),	FXC(-62462713ull, 24535873ull),
FXC(65567249ull, 14301591ull),	FXC(-14301591ull, 65567249ull),
FXC(36250294ull, 56475799ull),	FXC(-56475799ull, 36250294ull),
FXC(55103257ull, 38304447ull),	FXC(-38304447ull, 55103257ull),
FXC(11878552ull, 66049221ull),	FXC(-66049221ull, 11878552ull),
FXC(66653326ull, 7806011ull),	FXC(-7806011ull, 66653326ull),
FXC(41611335ull, 52650702ull),	FXC(-52650702ull, 41611335ull),
FXC(58592412ull, 32718937ull),	FXC(-32718937ull, 58592412ull),
FXC(18295309ull, 64566874ull),	FXC(-64566874ull, 18295309ull),
FXC(63849723ull, 20659440ull),	FXC(-20659440ull, 63849723ull),
FXC(30540143ull, 59757002ull),	FXC(-59757002ull, 30540143ull),
FXC(51083427ull, 43521065ull),	FXC(-43521065ull, 51083427ull),
FXC(5347398ull, 66895478ull),	FXC(-66895478ull, 5347398ull),
FXC(66956062ull, 4526085ull),	FXC(-4526085ull, 66956062ull),
FXC(44144660ull, 50545510ull),	FXC(-50545510ull, 44144660ull),
FXC(60127277ull, 29804533ull),	FXC(-29804533ull, 60127277ull),
FXC(21441418ull, 63591393ull),	FXC(-63591393ull, 21441418ull),
FXC(64786524ull, 17501597ull),	FXC(-17501597ull, 64786524ull),
FXC(33435493ull, 58186489ull),	FXC(-58186489ull, 33435493ull),
FXC(53157373ull, 40962097ull),	FXC(-40962097ull, 53157373ull),
FXC(8623362ull, 66552515ull),	FXC(-66552515ull, 8623362ull),
FXC(66190016ull, 11067132ull),	FXC(-11067132ull, 66190016ull),
FXC(38977765ull, 54629053ull),	FXC(-54629053ull, 38977765ull),
FXC(56916393ull, 35554519ull),	FXC(-35554519ull, 56916393ull),
FXC(15105126ull, 65386809ull),	FXC(-65386809ull, 15105126ull),
FXC(62759103ull, 23767512ull),	FXC(-23767512ull, 62759103ull),
FXC(27571218ull, 61183556ull),	FXC(-61183556ull, 27571218ull),
FXC(48886418ull, 45975187ull),	FXC(-45975187ull, 48886418ull),
FXC(2058551ull, 67077284ull),	FXC(-67077284ull, 2058551ull),
FXC(67046971ull, 2881538ull),	FXC(-2881538ull, 67046971ull),
FXC(45371813ull, 49446923ull),	FXC(-49446923ull, 45371813ull),
FXC(60840608ull, 28319959ull),	FXC(-28319959ull, 60840608ull),
FXC(22995571ull, 63046041ull),	FXC(-63046041ull, 22995571ull),
FXC(65196522ull, 15906385ull),	FXC(-15906385ull, 65196522ull),
FXC(34853391ull, 57348416ull),	FXC(-57348416ull, 34853391ull),
FXC(54146623ull, 39645212ull),	FXC(-39645212ull, 54146623ull),
FXC(10254046ull, 66320843ull),	FXC(-66320843ull, 10254046ull),
FXC(66441682ull, 9439415ull),	FXC(-9439415ull, 66441682ull),
FXC(40306690ull, 53656038ull),	FXC(-53656038ull, 40306690ull),
FXC(57771802ull, 34147013ull),	FXC(-34147013ull, 57771802ull),
FXC(16705249ull, 64996418ull),	FXC(-64996418ull, 16705249ull),
FXC(63323485ull, 22220168ull),	FXC(-22220168ull, 63323485ull),
FXC(29064434ull, 60488497ull),	FXC(-60488497ull, 29064434ull),
FXC(49999982ull, 44761607ull),	FXC(-44761607ull, 49999982ull),
FXC(3704090ull, 67006562ull),	FXC(-67006562ull, 3704090ull),
FXC(66824820ull, 6167906ull),	FXC(-6167906ull, 66824820ull),
FXC(42890915ull, 51613651ull),	FXC(-51613651ull, 42890915ull),
FXC(59377728ull, 31271153ull),	FXC(-31271153ull, 59377728ull),
FXC(19874350ull, 64098439ull),	FXC(-64098439ull, 19874350ull),
FXC(64337501ull, 19086267ull),	FXC(-19086267ull, 64337501ull),
FXC(31997455ull, 58989512ull),	FXC(-58989512ull, 31997455ull),
FXC(52136102ull, 42254307ull),	FXC(-42254307ull, 52136102ull),
FXC(6987485ull, 66744099ull),	FXC(-66744099ull, 6987485ull),
FXC(65898480ull, 12688183ull),	FXC(-12688183ull, 65898480ull),
FXC(37625361ull, 55569162ull),	FXC(-55569162ull, 37625361ull),
FXC(56026699ull, 36940609ull),	FXC(-36940609ull, 56026699ull),
FXC(13495904ull, 65737814ull),	FXC(-65737814ull, 13495904ull),
FXC(62156917ull, 25300539ull),	FXC(-25300539ull, 62156917ull),
FXC(26061395ull, 61841760ull),	FXC(-61841760ull, 26061395ull),
FXC(47743406ull, 47161073ull),	FXC(-47161073ull, 47743406ull),
FXC(411772ull, 67107601ull),	FXC(-67107601ull, 411772ull),
FXC(67108548ull, 205887ull),	FXC(-205887ull, 67108548ull),
FXC(47307325ull, 47598494ull),	FXC(-47598494ull, 47307325ull),
FXC(61921425ull, 25871544ull),	FXC(-25871544ull, 61921425ull),
FXC(25491115ull, 62079004ull),	FXC(-62079004ull, 25491115ull),
FXC(65778910ull, 13294159ull),	FXC(-13294159ull, 65778910ull),
FXC(37112323ull, 55913103ull),	FXC(-55913103ull, 37112323ull),
FXC(55684334ull, 37454701ull),	FXC(-37454701ull, 55684334ull),
FXC(12890297ull, 65859243ull),	FXC(-65859243ull, 12890297ull),
FXC(66765222ull, 6782684ull),	FXC(-6782684ull, 66765222ull),
FXC(42414059ull, 52006223ull),	FXC(-52006223ull, 42414059ull),
FXC(59087401ull, 31816327ull),	FXC(-31816327ull, 59087401ull),
FXC(19283562ull, 64278642ull),	FXC(-64278642ull, 19283562ull),
FXC(64159111ull, 19677605ull),	FXC(-19677605ull, 64159111ull),
FXC(31453174ull, 59281510ull),	FXC(-59281510ull, 31453174ull),
FXC(51744996ull, 42732365ull),	FXC(-42732365ull, 51744996ull),
FXC(6372892ull, 66805583ull),	FXC(-66805583ull, 6372892ull),
FXC(67017611ull, 3498499ull),	FXC(-3498499ull, 67017611ull),
FXC(44914794ull, 49862420ull),	FXC(-49862420ull, 44914794ull),
FXC(60577381ull, 28878721ull),	FXC(-28878721ull, 60577381ull),
FXC(22414337ull, 63255017ull),	FXC(-63255017ull, 22414337ull),
FXC(65047363ull, 16505764ull),	FXC(-16505764ull, 65047363ull),
FXC(34324094ull, 57666769ull),	FXC(-57666769ull, 34324094ull),
FXC(53779445ull, 40141886ull),	FXC(-40141886ull, 53779445ull),
FXC(9643210ull, 66412409ull),	FXC(-66412409ull, 9643210ull),
FXC(66351990ull, 10050528ull),	FXC(-10050528ull, 66351990ull),
FXC(39811145ull, 54024738ull),	FXC(-54024738ull, 39811145ull),
FXC(57455075ull, 34677284ull),	FXC(-34677284ull, 57455075ull),
FXC(16106330ull, 65147416ull),	FXC(-65147416ull, 16106330ull),
FXC(63116294ull, 22802041ull),	FXC(-22802041ull, 63116294ull),
FXC(28506482ull, 60753437ull),	FXC(-60753437ull, 28506482ull),
FXC(49585889ull, 45219899ull),	FXC(-45219899ull, 49585889ull),
FXC(3087221ull, 67037815ull),	FXC(-67037815ull, 3087221ull),
FXC(67083284ull, 1852751ull),	FXC(-1852751ull, 67083284ull),
FXC(46124952ull, 48745138ull),	FXC(-48745138ull, 46124952ull),
FXC(61267855ull, 27383380ull),	FXC(-27383380ull, 61267855ull),
FXC(23959942ull, 62685890ull),	FXC(-62685890ull, 23959942ull),
FXC(65432843ull, 14904451ull),	FXC(-14904451ull, 65432843ull),
FXC(35728969ull, 56807045ull),	FXC(-56807045ull, 35728969ull),
FXC(54748378ull, 38809982ull),	FXC(-38809982ull, 54748378ull),
FXC(11270148ull, 66155751ull),	FXC(-66155751ull, 11270148ull),
FXC(66578658ull, 8419141ull),	FXC(-8419141ull, 66578658ull),
FXC(41124988ull, 53031452ull),	FXC(-53031452ull, 41124988ull),
FXC(58288793ull, 33256822ull),	FXC(-33256822ull, 58288793ull),
FXC(17700277ull, 64732525ull),	FXC(-64732525ull, 17700277ull),
FXC(63656875ull, 21246222ull),	FXC(-21246222ull, 63656875ull),
FXC(29988860ull, 60035555ull),	FXC(-60035555ull, 29988860ull),
FXC(50680706ull, 43989381ull),	FXC(-43989381ull, 50680706ull),
FXC(4731482ull, 66941861ull),	FXC(-66941861ull, 4731482ull),
FXC(66911569ull, 5142140ull),	FXC(-5142140ull, 66911569ull),
FXC(43677582ull, 50949666ull),	FXC(-50949666ull, 43677582ull),
FXC(59850417ull, 30356667ull),	FXC(-30356667ull, 59850417ull),
FXC(20855231ull, 63786041ull),	FXC(-63786041ull, 20855231ull),
FXC(64622700ull, 18097135ull),	FXC(-18097135ull, 64622700ull),
FXC(32898542ull, 58491756ull),	FXC(-58491756ull, 32898542ull),
FXC(52778116ull, 41449609ull),	FXC(-41449609ull, 52778116ull),
FXC(8010464ull, 66629063ull),	FXC(-66629063ull, 8010464ull),
FXC(66085353ull, 11675860ull),	FXC(-11675860ull, 66085353ull),
FXC(38473322ull, 54985481ull),	FXC(-54985481ull, 38473322ull),
FXC(56586747ull, 36076858ull),	FXC(-36076858ull, 56586747ull),
FXC(14502682ull, 65523064ull),	FXC(-65523064ull, 14502682ull),
FXC(62537694ull, 24344125ull),	FXC(-24344125ull, 62537694ull),
FXC(27006932ull, 61434723ull),	FXC(-61434723ull, 27006932ull),
FXC(48461203ull, 46423178ull),	FXC(-46423178ull, 48461203ull),
FXC(1441101ull, 67093389ull),	FXC(-67093389ull, 1441101ull),
FXC(67100968ull, 1029397ull),	FXC(-1029397ull, 67100968ull),
FXC(46719656ull, 48175443ull),	FXC(-48175443ull, 46719656ull),
FXC(61599278ull, 26629467ull),	FXC(-26629467ull, 61599278ull),
FXC(24727390ull, 62387144ull),	FXC(-62387144ull, 24727390ull),
FXC(65610817ull, 14100367ull),	FXC(-14100367ull, 65610817ull),
FXC(36423389ull, 56364318ull),	FXC(-56364318ull, 36423389ull),
FXC(55220514ull, 38135213ull),	FXC(-38135213ull, 55220514ull),
FXC(12081132ull, 66012468ull),	FXC(-66012468ull, 12081132ull),
FXC(66676960ull, 7601485ull),	FXC(-7601485ull, 66676960ull),
FXC(41772669ull, 52522792ull),	FXC(-52522792ull, 41772669ull),
FXC(58692517ull, 32539024ull),	FXC(-32539024ull, 58692517ull),
FXC(18493312ull, 64510441ull),	FXC(-64510441ull, 18493312ull),
FXC(63912805ull, 20463454ull),	FXC(-20463454ull, 63912805ull),
FXC(30723331ull, 59663025ull),	FXC(-59663025ull, 30723331ull),
FXC(51216708ull, 43364138ull),	FXC(-43364138ull, 51216708ull),
FXC(5552605ull, 66878757ull),	FXC(-66878757ull, 5552605ull),
FXC(66969632ull, 4320645ull),	FXC(-4320645ull, 66969632ull),
FXC(44299524ull, 50409839ull),	FXC(-50409839ull, 44299524ull),
FXC(60218433ull, 29619924ull),	FXC(-29619924ull, 60218433ull),
FXC(21636413ull, 63525312ull),	FXC(-63525312ull, 21636413ull),
FXC(64839914ull, 17302752ull),	FXC(-17302752ull, 64839914ull),
FXC(33613849ull, 58083636ull),	FXC(-58083636ull, 33613849ull),
FXC(53282792ull, 40798819ull),	FXC(-40798819ull, 53282792ull),
FXC(8827502ull, 66525746ull),	FXC(-66525746ull, 8827502ull),
FXC(66223658ull, 10864012ull),	FXC(-10864012ull, 66223658ull),
FXC(39145181ull, 54509214ull),	FXC(-54509214ull, 39145181ull),
FXC(57025205ull, 35379735ull),	FXC(-35379735ull, 57025205ull),
FXC(15305658ull, 65340160ull),	FXC(-65340160ull, 15305658ull),
FXC(62831725ull, 23574858ull),	FXC(-23574858ull, 62831725ull),
FXC(27758797ull, 61098681ull),	FXC(-61098681ull, 27758797ull),
FXC(49027238ull, 45824989ull),	FXC(-45824989ull, 49027238ull),
FXC(2264332ull, 67070653ull),	FXC(-67070653ull, 2264332ull),
FXC(67055496ull, 2675827ull),	FXC(-2675827ull, 67055496ull),
FXC(45523301ull, 49307491ull),	FXC(-49307491ull, 45523301ull),
FXC(60927206ull, 28133169ull),	FXC(-28133169ull, 60927206ull),
FXC(23188886ull, 62975195ull),	FXC(-62975195ull, 23188886ull),
FXC(65245016ull, 15706290ull),	FXC(-15706290ull, 65245016ull),
FXC(35029169ull, 57241217ull),	FXC(-57241217ull, 35029169ull),
FXC(54267998ull, 39478906ull),	FXC(-39478906ull, 54267998ull),
FXC(10457467ull, 66289072ull),	FXC(-66289072ull, 10457467ull),
FXC(66470329ull, 9235530ull),	FXC(-9235530ull, 66470329ull),
FXC(40471114ull, 53532126ull),	FXC(-53532126ull, 40471114ull),
FXC(57876292ull, 33969611ull),	FXC(-33969611ull, 57876292ull),
FXC(16904576ull, 64944861ull),	FXC(-64944861ull, 16904576ull),
FXC(63391358ull, 22025790ull),	FXC(-22025790ull, 63391358ull),
FXC(29249873ull, 60399044ull),	FXC(-60399044ull, 29249873ull),
FXC(50137073ull, 44607999ull),	FXC(-44607999ull, 50137073ull),
FXC(3909646ull, 66994883ull),	FXC(-66994883ull, 3909646ull),
FXC(66843428ull, 5962861ull),	FXC(-5962861ull, 66843428ull),
FXC(43049062ull, 51481821ull),	FXC(-51481821ull, 43049062ull),
FXC(59473387ull, 31088838ull),	FXC(-31088838ull, 59473387ull),
FXC(20070907ull, 64037163ull),	FXC(-64037163ull, 20070907ull),
FXC(64395754ull, 18888792ull),	FXC(-18888792ull, 64395754ull),
FXC(32178281ull, 58891068ull),	FXC(-58891068ull, 32178281ull),
FXC(52265491ull, 42094157ull),	FXC(-42094157ull, 52265491ull),
FXC(7192220ull, 66722347ull),	FXC(-66722347ull, 7192220ull),
FXC(65937096ull, 12485950ull),	FXC(-12485950ull, 65937096ull),
FXC(37795668ull, 55453468ull),	FXC(-55453468ull, 37795668ull),
FXC(56139768ull, 36768548ull),	FXC(-36768548ull, 56139768ull),
FXC(13697521ull, 65696100ull),	FXC(-65696100ull, 13697521ull),
FXC(62234246ull, 25109725ull),	FXC(-25109725ull, 62234246ull),
FXC(26251000ull, 61761514ull),	FXC(-61761514ull, 26251000ull),
FXC(47887870ull, 47014376ull),	FXC(-47014376ull, 47887870ull),
FXC(617654ull, 67106022ull),	FXC(-67106022ull, 617654ull),
FXC(67106022ull, 617654ull),	FXC(-617654ull, 67106022ull),
FXC(47014376ull, 47887870ull),	FXC(-47887870ull, 47014376ull),
FXC(61761514ull, 26251000ull),	FXC(-26251000ull, 61761514ull),
FXC(25109725ull, 62234246ull),	FXC(-62234246ull, 25109725ull),
FXC(65696100ull, 13697521ull),	FXC(-13697521ull, 65696100ull),
FXC(36768548ull, 56139768ull),	FXC(-56139768ull, 36768548ull),
FXC(55453468ull, 37795668ull),	FXC(-37795668ull, 55453468ull),
FXC(12485950ull, 65937096ull),	FXC(-65937096ull, 12485950ull),
FXC(66722347ull, 7192220ull),	FXC(-7192220ull, 66722347ull),
FXC(42094157ull, 52265491ull),	FXC(-52265491ull, 42094157ull),
FXC(58891068ull, 32178281ull),	FXC(-32178281ull, 58891068ull),
FXC(18888792ull, 64395754ull),	FXC(-64395754ull, 18888792ull),
FXC(64037163ull, 20070907ull),	FXC(-20070907ull, 64037163ull),
FXC(31088838ull, 59473387ull),	FXC(-59473387ull, 31088838ull),
FXC(51481821ull, 43049062ull),	FXC(-43049062ull, 51481821ull),
FXC(5962861ull, 66843428ull),	FXC(-66843428ull, 5962861ull),
FXC(66994883ull, 3909646ull),	FXC(-3909646ull, 66994883ull),
FXC(44607999ull, 50137073ull),	FXC(-50137073ull, 44607999ull),
FXC(60399044ull, 29249873ull),	FXC(-29249873ull, 60399044ull),
FXC(22025790ull, 63391358ull),	FXC(-63391358ull, 22025790ull),
FXC(64944861ull, 16904576ull),	FXC(-16904576ull, 64944861ull),
FXC(33969611ull, 57876292ull),	FXC(-57876292ull, 33969611ull),
FXC(53532126ull, 40471114ull),	FXC(-40471114ull, 53532126ull),
FXC(9235530ull, 66470329ull),	FXC(-66470329ull, 9235530ull),
FXC(66289072ull, 10457467ull),	FXC(-10457467ull, 66289072ull),
FXC(39478906ull, 54267998ull),	FXC(-54267998ull, 39478906ull),
FXC(57241217ull, 35029169ull),	FXC(-35029169ull, 57241217ull),
FXC(15706290ull, 65245016ull),	FXC(-65245016ull, 15706290ull),
FXC(62975195ull, 23188886ull),	FXC(-23188886ull, 62975195ull),
FXC(28133169ull, 60927206ull),	FXC(-60927206ull, 28133169ull),
FXC(49307491ull, 45523301ull),	FXC(-45523301ull, 49307491ull),
FXC(2675827ull, 67055496ull),	FXC(-67055496ull, 2675827ull),
FXC(67070653ull, 2264332ull),	FXC(-2264332ull, 67070653ull),
FXC(45824989ull, 49027238ull),	FXC(-49027238ull, 45824989ull),
FXC(61098681ull, 27758797ull),	FXC(-27758797ull, 61098681ull),
FXC(23574858ull, 62831725ull),	FXC(-62831725ull, 23574858ull),
FXC(65340160ull, 15305658ull),	FXC(-15305658ull, 65340160ull),
FXC(35379735ull, 57025205ull),	FXC(-57025205ull, 35379735ull),
FXC(54509214ull, 39145181ull),	FXC(-39145181ull, 54509214ull),
FXC(10864012ull, 66223658ull),	FXC(-66223658ull, 10864012ull),
FXC(66525746ull, 8827502ull),	FXC(-8827502ull, 66525746ull),
FXC(40798819ull, 53282792ull),	FXC(-53282792ull, 40798819ull),
FXC(58083636ull, 33613849ull),	FXC(-33613849ull, 58083636ull),
FXC(17302752ull, 64839914ull),	FXC(-64839914ull, 17302752ull),
FXC(63525312ull, 21636413ull),	FXC(-21636413ull, 63525312ull),
FXC(29619924ull, 60218433ull),	FXC(-60218433ull, 29619924ull),
FXC(50409839ull, 44299524ull),	FXC(-44299524ull, 50409839ull),
FXC(4320645ull, 66969632ull),	FXC(-66969632ull, 4320645ull),
FXC(66878757ull, 5552605ull),	FXC(-5552605ull, 66878757ull),
FXC(43364138ull, 51216708ull),	FXC(-51216708ull, 43364138ull),
FXC(59663025ull, 30723331ull),	FXC(-30723331ull, 59663025ull),
FXC(20463454ull, 63912805ull),	FXC(-63912805ull, 20463454ull),
FXC(64510441ull, 18493312ull),	FXC(-18493312ull, 64510441ull),
FXC(32539024ull, 58692517ull),	FXC(-58692517ull, 32539024ull),
FXC(52522792ull, 41772669ull),	FXC(-41772669ull, 52522792ull),
FXC(7601485ull, 66676960ull),	FXC(-66676960ull, 7601485ull),
FXC(66012468ull, 12081132ull),	FXC(-12081132ull, 66012468ull),
FXC(38135213ull, 55220514ull),	FXC(-55220514ull, 38135213ull),
FXC(56364318ull, 36423389ull),	FXC(-36423389ull, 56364318ull),
FXC(14100367ull, 65610817ull),	FXC(-65610817ull, 14100367ull),
FXC(62387144ull, 24727390ull),	FXC(-24727390ull, 62387144ull),
FXC(26629467ull, 61599278ull),	FXC(-61599278ull, 26629467ull),
FXC(48175443ull, 46719656ull),	FXC(-46719656ull, 48175443ull),
FXC(1029397ull, 67100968ull),	FXC(-67100968ull, 1029397ull),
FXC(67093389ull, 1441101ull),	FXC(-1441101ull, 67093389ull),
FXC(46423178ull, 48461203ull),	FXC(-48461203ull, 46423178ull),
FXC(61434723ull, 27006932ull),	FXC(-27006932ull, 61434723ull),
FXC(24344125ull, 62537694ull),	FXC(-62537694ull, 24344125ull),
FXC(65523064ull, 14502682ull),	FXC(-14502682ull, 65523064ull),
FXC(36076858ull, 56586747ull),	FXC(-56586747ull, 36076858ull),
FXC(54985481ull, 38473322ull),	FXC(-38473322ull, 54985481ull),
FXC(11675860ull, 66085353ull),	FXC(-66085353ull, 11675860ull),
FXC(66629063ull, 8010464ull),	FXC(-8010464ull, 66629063ull),
FXC(41449609ull, 52778116ull),	FXC(-52778116ull, 41449609ull),
FXC(58491756ull, 32898542ull),	FXC(-32898542ull, 58491756ull),
FXC(18097135ull, 64622700ull),	FXC(-64622700ull, 18097135ull),
FXC(63786041ull, 20855231ull),	FXC(-20855231ull, 63786041ull),
FXC(30356667ull, 59850417ull),	FXC(-59850417ull, 30356667ull),
FXC(50949666ull, 43677582ull),	FXC(-43677582ull, 50949666ull),
FXC(5142140ull, 66911569ull),	FXC(-66911569ull, 5142140ull),
FXC(66941861ull, 4731482ull),	FXC(-4731482ull, 66941861ull),
FXC(43989381ull, 50680706ull),	FXC(-50680706ull, 43989381ull),
FXC(60035555ull, 29988860ull),	FXC(-29988860ull, 60035555ull),
FXC(21246222ull, 63656875ull),	FXC(-63656875ull, 21246222ull),
FXC(64732525ull, 17700277ull),	FXC(-17700277ull, 64732525ull),
FXC(33256822ull, 58288793ull),	FXC(-58288793ull, 33256822ull),
FXC(53031452ull, 41124988ull),	FXC(-41124988ull, 53031452ull),
FXC(8419141ull, 66578658ull),	FXC(-66578658ull, 8419141ull),
FXC(66155751ull, 11270148ull),	FXC(-11270148ull, 66155751ull),
FXC(38809982ull, 54748378ull),	FXC(-54748378ull, 38809982ull),
FXC(56807045ull, 35728969ull),	FXC(-35728969ull, 56807045ull),
FXC(14904451ull, 65432843ull),	FXC(-65432843ull, 14904451ull),
FXC(62685890ull, 23959942ull),	FXC(-23959942ull, 62685890ull),
FXC(27383380ull, 61267855ull),	FXC(-61267855ull, 27383380ull),
FXC(48745138ull, 46124952ull),	FXC(-46124952ull, 48745138ull),
FXC(1852751ull, 67083284ull),	FXC(-67083284ull, 1852751ull),
FXC(67037815ull, 3087221ull),	FXC(-3087221ull, 67037815ull),
FXC(45219899ull, 49585889ull),	FXC(-49585889ull, 45219899ull),
FXC(60753437ull, 28506482ull),	FXC(-28506482ull, 60753437ull),
FXC(22802041ull, 63116294ull),	FXC(-63116294ull, 22802041ull),
FXC(65147416ull, 16106330ull),	FXC(-16106330ull, 65147416ull),
FXC(34677284ull, 57455075ull),	FXC(-57455075ull, 34677284ull),
FXC(54024738ull, 39811145ull),	FXC(-39811145ull, 54024738ull),
FXC(10050528ull, 66351990ull),	FXC(-66351990ull, 10050528ull),
FXC(66412409ull, 9643210ull),	FXC(-9643210ull, 66412409ull),
FXC(40141886ull, 53779445ull),	FXC(-53779445ull, 40141886ull),
FXC(57666769ull, 34324094ull),	FXC(-34324094ull, 57666769ull),
FXC(16505764ull, 65047363ull),	FXC(-65047363ull, 16505764ull),
FXC(63255017ull, 22414337ull),	FXC(-22414337ull, 63255017ull),
FXC(28878721ull, 60577381ull),	FXC(-60577381ull, 28878721ull),
FXC(49862420ull, 44914794ull),	FXC(-44914794ull, 49862420ull),
FXC(3498499ull, 67017611ull),	FXC(-67017611ull, 3498499ull),
FXC(66805583ull, 6372892ull),	FXC(-6372892ull, 66805583ull),
FXC(42732365ull, 51744996ull),	FXC(-51744996ull, 42732365ull),
FXC(59281510ull, 31453174ull),	FXC(-31453174ull, 59281510ull),
FXC(19677605ull, 64159111ull),	FXC(-64159111ull, 19677605ull),
FXC(64278642ull, 19283562ull),	FXC(-19283562ull, 64278642ull),
FXC(31816327ull, 59087401ull),	FXC(-59087401ull, 31816327ull),
FXC(52006223ull, 42414059ull),	FXC(-42414059ull, 52006223ull),
FXC(6782684ull, 66765222ull),	FXC(-66765222ull, 6782684ull),
FXC(65859243ull, 12890297ull),	FXC(-12890297ull, 65859243ull),
FXC(37454701ull, 55684334ull),	FXC(-55684334ull, 37454701ull),
FXC(55913103ull, 37112323ull),	FXC(-37112323ull, 55913103ull),
FXC(13294159ull, 65778910ull),	FXC(-65778910ull, 13294159ull),
FXC(62079004ull, 25491115ull),	FXC(-25491115ull, 62079004ull),
FXC(25871544ull, 61921425ull),	FXC(-61921425ull, 25871544ull),
FXC(47598494ull, 47307325ull),	FXC(-47307325ull, 47598494ull),
FXC(205887ull, 67108548ull),	FXC(-67108548ull, 205887ull),


};

// precomputed powers of 2 (2^1, 2^0, 2^-1, 2^-2, ..., 2^-9)
const fxr fxr_p2_tab[] = {
	{0x0000000008000000},
	{0x0000000004000000 },
	{0x0000000002000000 },
	{0x0000000001000000 },
	{0x0000000000800000 },
	{0x0000000000400000 },
	{0x0000000000200000 },
	{0x0000000000100000 },
	{0x0000000000080000 },
	{0x0000000000040000 },
	{0x0000000000020000 }

};

// 1/sigma
const fxr fxr_inv_sigma[] = {
	{0 }	,
	{463419 },
	{457027 },
	{450892 },
	{442136 },
	{433871 },
	{426053 },
	{418642 },
	{411606 },
	{404913 },
	{398536 },

};

// 1/sigma_min
const fxr fxr_sigma_min[] = {
	//{ 0x0000000000000000 }	,
	{ 0x0000000000000000 }	,
	{ 0x0000000004774e01 }	,
	{ 0x0000000004874bb7 }	,
	{ 0x00000000049711b8 }	,
	{ 0x0000000004ae5715 }	,
	{ 0x0000000004c52af1 }	,
	{ 0x0000000004db938d }	,
	{ 0x0000000004f19699 }	,
	{ 0x000000000507394d  }	,
	{ 0x00000000051c806f  }	,
	{ 0x0000000005317066 }	,
};

const fxr fxr_q = { 0x000000c004000000 };				// q = 12289
const fxr fxr_inverse_of_q = { 0x0000000000001555 };	// 1/12289
const fxr fxr_inv_2sqrsigma0 = { 0x00000000009a7c5e };	// 1/(2*(1.8205^2))
const fxr fxr_log2 = { 0x2C5C860 };						// ln2 = 0,69314718055994530941723212145818
const fxr fxr_inv_log2 = { 0x0000000005c551d9 };		// 1/ln2
const fxr fxr_bnorm_max = { 0x00000106d9a5fd8a };		// norma max value (16822.4121)
const fxr fxr_invsqrt2 = { 0x0000000002d413cd };		// 1/sqrt(2)= 0,70710678118654752440084436210485
const fxr fxr_invsqrt8 = { 0x00000000016a09e6 };		// 1/sqrt(8)= 0,35355339059327376220042218105242

const fxr fxr_zero = { 0 };
const fxr fxr_sqrt2 = { 94906265ull };

// Calculation exp(x) for 0 <=x < 1
// changes:
// scale = 2 * SCALE
// Constants for scale = 2 * SCALE and function 
static fxr2 fxr2_exp_c[] = {
	{9339440},
	{113938847},
	{1241225186},
	{12410057660},
	{111696327149},
	{893571538674},
	{6254999505761},
	{37529996869837},
	{187649984471265},
	{750599937896511},
	{2251799813685334},
	{4503599627370473},
	{4503599627370496}
};

fxr2 fxr2_exp_(fxr2 x)
{
	//x.v = 0.055750638246536255 * ((uint64_t)1 << 2 * SCALE);
	fxr2 y = fxr2_exp_c[0], d = x;
	//y = 0.000000025299506379442070029551 - y * d;
	y = fxr2_sub(fxr2_exp_c[1], fxr2_mul(y, d));
	//y = 0.000000275607356160477811864927 - y * d;
	y = fxr2_sub(fxr2_exp_c[2], fxr2_mul(y, d));
	//y = 0.000002755586350219122514855659 - y * d;
	y = fxr2_sub(fxr2_exp_c[3], fxr2_mul(y, d));
	//y = 0.000024801566833585381209939524 - y * d;
	y = fxr2_sub(fxr2_exp_c[4], fxr2_mul(y, d));
	//y = 0.000198412739277311890541063977 - y * d;
	y = fxr2_sub(fxr2_exp_c[5], fxr2_mul(y, d));
	//y = 0.001388888894063186997887560103 - y * d;
	y = fxr2_sub(fxr2_exp_c[6], fxr2_mul(y, d));

	//y = 0.008333333327800835146903501993 - y * d;
	y = fxr2_sub(fxr2_exp_c[7], fxr2_mul(y, d));
	//y = 0.041666666666110491190622155955 - y * d;
	y = fxr2_sub(fxr2_exp_c[8], fxr2_mul(y, d));
	//y = 0.166666666666984014666397229121 - y * d;
	y = fxr2_sub(fxr2_exp_c[9], fxr2_mul(y, d));
	//y = 0.500000000000019206858326015208 - y * d;
	y = fxr2_sub(fxr2_exp_c[10], fxr2_mul(y, d));
	//y = 0.999999999999994892974086724280 - y * d;
	y = fxr2_sub(fxr2_exp_c[11], fxr2_mul(y, d));
	//y = 1.000000000000000000000000000000 - y * d;
	y = fxr2_sub(fxr2_exp_c[12], fxr2_mul(y, d));
	return y;
}



void
vect_set(unsigned logn, fxr* d, const int8_t* f)
{
	size_t n = (size_t)1 << logn;

	for (size_t u = 0; u < n; u++) {
		d[u] = fxr_of(f[u]);
	}
}

void
vect_neg(unsigned logn, fxr* d)
{
	size_t n = (size_t)1 << logn;

	for (size_t u = 0; u < n; u++) {
		d[u] = fxr_neg(d[u]);
	}
}

void
vect_FFT(unsigned logn, fxr* f)
{
	size_t hn = (size_t)1 << (logn - 1);
	size_t t = hn;

	for (unsigned lm = 1; lm < logn; lm++) {
		size_t m = (size_t)1 << lm;
		size_t ht = t >> 1;
		size_t j0 = 0;
		size_t hm = m >> 1;


		for (size_t i = 0; i < hm; i++) {


			fxc s = GM_TAB[m + i];
			
			for (size_t j = j0; j < j0 + ht; j++) {

			
			
				fxc x, y;
			
			
				x.re = f[j];
				x.im = f[j + hn];
				y.re = f[j + ht];
				y.im = f[j + ht + hn];
			
				y = fxc_mul(s, y);
			
				fxc z1 = fxc_add(x, y);
			
				f[j] = z1.re;
				f[j + hn] = z1.im;
				

				fxc z2 = fxc_sub(x, y);
				

				f[j + ht] = z2.re;
				f[j + ht + hn] = z2.im;
				
			}
			j0 += t;
		}


		t = ht;
	}
	
}


void
vect_iFFT(unsigned logn, fxr* f)
{
	size_t n = (size_t)1 << (logn);
	size_t hn = n / 2;
	size_t ht = 1;

	for (unsigned lm = logn - 1; lm > 0; lm--) {
		size_t m = (size_t)1 << lm;
		size_t t = ht << 1;
		size_t j0 = 0;
		size_t hm = m >> 1;
		for (size_t i = 0; i < hm; i++) {
			fxc s = fxc_conj(GM_TAB[m + i]);
			for (size_t j = j0; j < j0 + ht; j++) {

				fxc x, y;
				x.re = f[j];
				x.im = f[j + hn];
				y.re = f[j + ht];
				y.im = f[j + ht + hn];

				fxc z1 = fxc_add(x, y);
				f[j] = z1.re;
				f[j + hn] = z1.im;

				fxc z2 = fxc_mul(s, fxc_sub(x, y));
				f[j + ht] = z2.re;
				f[j + ht + hn] = z2.im;
			}
			j0 += t;
		}
		ht = t;
	}
	if (logn > 0) {
		fxr ni;

		ni = fxr_p2_tab[logn];
		for (size_t u = 0; u < n; u++) {
			f[u] = fxr_mul(f[u], ni);
		}
	}
}

void
vect_invnorm_fft(unsigned logn, fxr* restrict d,
	const fxr* restrict a, const fxr* restrict b, unsigned e)
{
	size_t hn = (size_t)1 << (logn - 1);
	fxr fe = fxr_of((int32_t)1 << e);
	for (size_t u = 0; u < hn; u++) {
		fxr z = fxr_add(
			fxr_add(fxr_sqr(a[u]), fxr_sqr(a[u + hn])),
			fxr_add(fxr_sqr(b[u]), fxr_sqr(b[u + hn])));
		d[u] = fxr_div(fe, z);
	}
}


void
vect_adj_fft(unsigned logn, fxr* a)
{
	size_t hn = (size_t)1 << (logn - 1);
	size_t n = (size_t)1 << logn;

	for (size_t u = hn; u < n; u++) {
		a[u] = fxr_neg(a[u]);
	}
}


void
vect_mul_realconst(unsigned logn, fxr* a, fxr c)
{
	size_t n = (size_t)1 << logn;

	for (size_t u = 0; u < n; u++) {
		a[u] = fxr_mul(a[u], c);
	}
}

void
vect_mul_autoadj_fft(unsigned logn, fxr* restrict a, const fxr* restrict b)
{
	size_t hn = (size_t)1 << (logn - 1);

	for (size_t u = 0; u < hn; u++) {
		a[u] = fxr_mul(a[u], b[u]);
		a[u + hn] = fxr_mul(a[u + hn], b[u]);
	}
}



void
vect_norm_fft(unsigned logn, fxr* restrict d,
	const fxr* restrict a, const fxr* restrict b)
{
	size_t hn = (size_t)1 << (logn - 1);

	for (size_t u = 0; u < hn; u++) {
		d[u] = fxr_add(
			fxr_add(fxr_sqr(a[u]), fxr_sqr(a[u + hn])),
			fxr_add(fxr_sqr(b[u]), fxr_sqr(b[u + hn])));
	}
}

void
vect_mul2e(unsigned logn, fxr* a, unsigned e)
{
	size_t n = (size_t)1 << logn;

	for (size_t u = 0; u < n; u++) {
		a[u] = fxr_mul2e(a[u], e);
	}
}

void
vect_mul_fft(unsigned logn, fxr* restrict a, const fxr* restrict b)
{
	size_t hn = (size_t)1 << (logn - 1);

	for (size_t u = 0; u < hn; u++) {
		fxc x, y;
		x.re = a[u];
		x.im = a[u + hn];
		y.re = b[u];
		y.im = b[u + hn];
		fxc z = fxc_mul(x, y);
		a[u] = z.re;
		a[u + hn] = z.im;
	}
}

void
vect_add(unsigned logn, fxr* restrict a, const fxr* restrict b)
{
	size_t n = (size_t)1 << logn;

	for (size_t u = 0; u < n; u++) {
		a[u] = fxr_add(a[u], b[u]);
	}
}

void
vect_sub(
	unsigned logn, fxr* restrict a, const fxr* restrict b)
{
	size_t n, u;

	n = (size_t)1 << logn;

	for (u = 0; u < n; u++) {
		a[u] = fxr_sub(a[u], b[u]);
	}

}



void
vect_div_autoadj_fft(unsigned logn, fxr* restrict a, const fxr* restrict b)
{
	size_t hn = (size_t)1 << (logn - 1);

	for (size_t u = 0; u < hn; u++) {
		a[u] = fxr_div(a[u], b[u]);
		a[u + hn] = fxr_div(a[u + hn], b[u]);
	}
}


void
vect_mulconst(unsigned logn, fxr* a, fxr x)
{
	size_t n, u;

	n = (size_t)1 << logn;

	for (u = 0; u < n; u++) {
		a[u] = fxr_mul(a[u], x);
	}

}

static inline int64_t
emu_double_FPR(int s, int e, uint64_t m)
{
	int64_t x;
	uint32_t t;
	unsigned f;

	/*
	 * If e >= -1076, then the value is "normal"; otherwise, it
	 * should be a subnormal, which we clamp down to zero.
	 */
	e += 1076;
	t = (uint32_t)e >> 31;
	m &= (uint64_t)t - 1;

	/*
	 * If m = 0 then we want a zero; make e = 0 too, but conserve
	 * the sign.
	 */
	t = (uint32_t)(m >> 54);
	e &= -(int)t;

	/*
	 * The 52 mantissa bits come from m. Value m has its top bit set
	 * (unless it is a zero); we leave it "as is": the top bit will
	 * increment the exponent by 1, except when m = 0, which is
	 * exactly what we want.
	 */
	x = (((uint64_t)s << 63) | (m >> 2)) + ((uint64_t)(uint32_t)e << 52);

	/*
	 * Rounding: if the low three bits of m are 011, 110 or 111,
	 * then the value should be incremented to get the next
	 * representable value. This implements the usual
	 * round-to-nearest rule (with preference to even values in case
	 * of a tie). Note that the increment may make a carry spill
	 * into the exponent field, which is again exactly what we want
	 * in that case.
	 */
	f = (unsigned)m & 7U;
	x += (0xC8U >> f) & 1;
	return x;
}


uint64_t
emu_double_sqrt(uint64_t x)
{
	uint64_t xu, q, s, r;
	int ex, e;

	/*
	 * Extract the mantissa and the exponent. We don't care about
	 * the sign: by assumption, the operand is nonnegative.
	 * We want the "true" exponent corresponding to a mantissa
	 * in the 1..2 range.
	 */
	xu = (x & (((uint64_t)1 << 52) - 1)) | ((uint64_t)1 << 52);
	ex = (int)((x >> 52) & 0x7FF);
	e = ex - 1023;

	/*
	 * If the exponent is odd, double the mantissa and decrement
	 * the exponent. The exponent is then halved to account for
	 * the square root.
	 */
	xu += xu & -(uint64_t)(e & 1);
	e >>= 1;

	/*
	 * Double the mantissa.
	 */
	xu <<= 1;

	/*
	 * We now have a mantissa in the 2^53..2^55-1 range. It
	 * represents a value between 1 (inclusive) and 4 (exclusive)
	 * in fixed point notation (with 53 fractional bits). We
	 * compute the square root bit by bit.
	 */
	q = 0;
	s = 0;
	r = (uint64_t)1 << 53;
	for (int i = 0; i < 54; i++) {
		uint64_t t, b;

		t = s + r;
		b = ((xu - t) >> 63) - 1;
		s += (r << 1) & b;
		xu -= t & b;
		q += r & b;
		xu <<= 1;
		r >>= 1;
	}

	/*
	 * Now, q is a rounded-low 54-bit value, with a leading 1,
	 * 52 fractional digits, and an additional guard bit. We add
	 * an extra sticky bit to account for what remains of the operand.
	 */
	q <<= 1;
	q |= (xu | -xu) >> 63;

	/*
	 * Result q is in the 2^54..2^55-1 range; we bias the exponent
	 * by 54 bits (the value e at that point contains the "true"
	 * exponent, but q is now considered an integer, i.e. scaled
	 * up.
	 */
	e -= 54;

	/*
	 * Corrective action for an operand of value zero.
	 */
	q &= -(uint64_t)((ex + 0x7FF) >> 11);

	/*
	 * Apply rounding and back result.
	 */
	return emu_double_FPR(0, e, q);
}

int test_fxr_sqrt()
{
	uint64_t start, finish, dif;
	uint64_t t_double_min , t_emu_double_min , 
		t_fxr_min , t_fxr1_min;
	uint64_t t_double_max = 0, t_emu_double_max = 0, t_fxr_max = 0, t_fxr1_max = 0;
	int success = 0;
	uint64_t xx [1024];
	for (int i = 0; i < 1024; ++i)
	{
		xx [i] = ((uint64_t)rand() << (62 - 15)) + ((uint64_t)rand() << (62 - 30)) +
			((uint64_t)rand() << (62 - 45)) + ((uint64_t)rand() << (62 - 60)) +
			((uint64_t)rand() & 3);
	}
	uint64_t x;
	for (int i = 0; i < 1024; ++i)
	{
		x = xx[i];
		double double_x = (x + 0.) / ((uint64_t)1 << SCALE), double_y;
		uint64_t emu_double_x = *(uint64_t*)&double_x, emu_double_y;
		fxr fxr_x, fxr_y1, fxr_y2; 
		fxr_x.v = x;
		
		
		t_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			double_y = sqrt(double_x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_double_min)
				t_double_min = dif;
		}
		if (t_double_min > t_double_max)
				t_double_max = t_double_min;

		t_fxr_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr_y1 = fxr_sqrt(fxr_x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr_min)
				t_fxr_min = dif;
		}
		if (t_fxr_min > t_fxr_max)
				t_fxr_max = dif;
		
		
		t_fxr1_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr_y2 = fxr_sqrt1(fxr_x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr1_min)
				t_fxr1_min = dif;
		}
		if (t_fxr1_min > t_fxr1_max)
			t_fxr1_max = t_fxr1_min;
				
		t_emu_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			emu_double_y = emu_double_sqrt(emu_double_x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_emu_double_min)
				t_emu_double_min = dif;
		}
		if (t_emu_double_min > t_emu_double_max)
				t_emu_double_max = t_emu_double_min;
		
		
		success |= (uint64_t)(double_y + 0.5) != (uint64_t)(*(double*)&emu_double_y+0.5);
		success |= (uint64_t)(double_y + 0.5) != (uint64_t)(((fxr_y1.v  + 0.)/ (1 << SCALE)) + 0.5);
		success |= (uint64_t)(double_y + 0.5) != (uint64_t)(((fxr_y2.v + 0.) / (1 << SCALE)) + 0.5);
	}
	printf("sqrt: t_double_min = %ld t_double_max = %ld\n", t_double_min, t_double_max);
	printf("sqrt: t_emu_double_min = %ld t_emu_double_max = %ld\n", t_emu_double_min, t_emu_double_max);
	printf("sqrt: t_fxr_min = %ld t_fxr_max = %ld\n", t_fxr_min, t_fxr_max);
	printf("sqrt: t_fxr1_min = %ld t_fxr1_max = %ld\n", t_fxr1_min, t_fxr1_max);
	return success;
}

uint64_t
emu_double_mul(int64_t x, int64_t y)
{
	uint64_t xu, yu, w, zu, zv;
	uint32_t x0, x1, y0, y1, z0, z1, z2;
	int ex, ey, d, e, s;

	/*
	 * Extract absolute values as scaled unsigned integers. We
	 * don't extract exponents yet.
	 */
	xu = (x & (((uint64_t)1 << 52) - 1)) | ((uint64_t)1 << 52);
	yu = (y & (((uint64_t)1 << 52) - 1)) | ((uint64_t)1 << 52);

	/*
	 * We have two 53-bit integers to multiply; we need to split
	 * each into a lower half and a upper half. Moreover, we
	 * prefer to have lower halves to be of 25 bits each, for
	 * reasons explained later on.
	 */
	x0 = (uint32_t)xu & 0x01FFFFFF;
	x1 = (uint32_t)(xu >> 25);
	y0 = (uint32_t)yu & 0x01FFFFFF;
	y1 = (uint32_t)(yu >> 25);
	w = (uint64_t)x0 * (uint64_t)y0;
	z0 = (uint32_t)w & 0x01FFFFFF;
	z1 = (uint32_t)(w >> 25);
	w = (uint64_t)x0 * (uint64_t)y1;
	z1 += (uint32_t)w & 0x01FFFFFF;
	z2 = (uint32_t)(w >> 25);
	w = (uint64_t)x1 * (uint64_t)y0;
	z1 += (uint32_t)w & 0x01FFFFFF;
	z2 += (uint32_t)(w >> 25);
	zu = (uint64_t)x1 * (uint64_t)y1;
	z2 += (z1 >> 25);
	z1 &= 0x01FFFFFF;
	zu += z2;

	/*
	 * Since xu and yu are both in the 2^52..2^53-1 range, the
	 * product is in the 2^104..2^106-1 range. We first reassemble
	 * it and round it into the 2^54..2^56-1 range; the bottom bit
	 * is made "sticky". Since the low limbs z0 and z1 are 25 bits
	 * each, we just take the upper part (zu), and consider z0 and
	 * z1 only for purposes of stickiness.
	 * (This is the reason why we chose 25-bit limbs above.)
	 */
	zu |= ((z0 | z1) + 0x01FFFFFF) >> 25;

	/*
	 * We normalize zu to the 2^54..s^55-1 range: it could be one
	 * bit too large at this point. This is done with a conditional
	 * right-shift that takes into account the sticky bit.
	 */
	zv = (zu >> 1) | (zu & 1);
	w = zu >> 55;
	zu ^= (zu ^ zv) & -w;

	/*
	 * Get the aggregate scaling factor:
	 *
	 *   - Each exponent is biased by 1023.
	 *
	 *   - Integral mantissas are scaled by 2^52, hence an
	 *     extra 52 bias for each exponent.
	 *
	 *   - However, we right-shifted z by 50 bits, and then
	 *     by 0 or 1 extra bit (depending on the value of w).
	 *
	 * In total, we must add the exponents, then subtract
	 * 2 * (1023 + 52), then add 50 + w.
	 */
	ex = (int)((x >> 52) & 0x7FF);
	ey = (int)((y >> 52) & 0x7FF);
	e = ex + ey - 2100 + (int)w;

	/*
	 * Sign bit is the XOR of the operand sign bits.
	 */
	s = (int)((x ^ y) >> 63);

	/*
	 * Corrective actions for zeros: if either of the operands is
	 * zero, then the computations above were wrong. Test for zero
	 * is whether ex or ey is zero. We just have to set the mantissa
	 * (zu) to zero, the FPR() function will normalize e.
	 */
	d = ((ex + 0x7FF) & (ey + 0x7FF)) >> 11;
	zu &= -(uint64_t)d;

	/*
	 * FPR() packs the result and applies proper rounding.
	 */
	
	return emu_double_FPR(s, e, zu);
}


int64_t emu_double_div(int64_t x, int64_t y)
{
	uint64_t xu, yu, q, q2, w;
	int i, ex, ey, e, d, s;

	/*
	 * Extract mantissas of x and y (unsigned).
	 */
	xu = (x & (((uint64_t)1 << 52) - 1)) | ((uint64_t)1 << 52);
	yu = (y & (((uint64_t)1 << 52) - 1)) | ((uint64_t)1 << 52);

	/*
	 * Perform bit-by-bit division of xu by yu. We run it for 55 bits.
	 */
	q = 0;
	for (i = 0; i < 55; i++) {
		/*
		 * If yu is less than or equal xu, then subtract it and
		 * push a 1 in the quotient; otherwise, leave xu unchanged
		 * and push a 0.
		 */
		uint64_t b;

		b = ((xu - yu) >> 63) - 1;
		xu -= b & yu;
		q |= b & 1;
		xu <<= 1;
		q <<= 1;
	}

	/*
	 * We got 55 bits in the quotient, followed by an extra zero. We
	 * want that 56th bit to be "sticky": it should be a 1 if and
	 * only if the remainder (xu) is non-zero.
	 */
	q |= (xu | -xu) >> 63;

	/*
	 * Quotient is at most 2^56-1. Its top bit may be zero, but in
	 * that case the next-to-top bit will be a one, since the
	 * initial xu and yu were both in the 2^52..2^53-1 range.
	 * We perform a conditional shift to normalize q to the
	 * 2^54..2^55-1 range (with the bottom bit being sticky).
	 */
	q2 = (q >> 1) | (q & 1);
	w = q >> 55;
	q ^= (q ^ q2) & -w;

	/*
	 * Extract exponents to compute the scaling factor:
	 *
	 *   - Each exponent is biased and we scaled them up by
	 *     52 bits; but these biases will cancel out.
	 *
	 *   - The division loop produced a 55-bit shifted result,
	 *     so we must scale it down by 55 bits.
	 *
	 *   - If w = 1, we right-shifted the integer by 1 bit,
	 *     hence we must add 1 to the scaling.
	 */
	ex = (int)((x >> 52) & 0x7FF);
	ey = (int)((y >> 52) & 0x7FF);
	e = ex - ey - 55 + (int)w;

	/*
	 * Sign is the XOR of the signs of the operands.
	 */
	s = (int)((x ^ y) >> 63);

	/*
	 * Corrective actions for zeros: if x = 0, then the computation
	 * is wrong, and we must clamp e and q to 0. We do not care
	 * about the case y = 0 (as per assumptions in this module,
	 * the caller does not perform divisions by zero).
	 */
	d = (ex + 0x7FF) >> 11;
	s &= d;
	e &= -d;
	q &= -(uint64_t)d;

	/*
	 * FPR() packs the result and applies proper rounding.
	 */
	return emu_double_FPR(s, e, q);

}

int64_t fxr32_mul(int64_t x, int64_t y)
{
	int32_t xh, yh;
	uint32_t xl, yl;
	uint64_t z0, z1, z2, z3;

	xl = (uint32_t)x;
	yl = (uint32_t)y;
	xh = (int32_t)(*(int64_t*)&x >> 32);
	yh = (int32_t)(*(int64_t*)&y >> 32);
	z0 = ((uint64_t)xl * (uint64_t)yl) >> 32;
	z1 = (uint64_t)((int64_t)xl * (int64_t)yh);
	z2 = (uint64_t)((int64_t)yl * (int64_t)xh);
	z3 = (uint64_t)((int64_t)xh * (int64_t)yh) << 32;
	x = z0 + z1 + z2 + z3;
	return x;

}

uint64_t
inner_fxr32_div(uint64_t x, uint64_t y)
{
	/*
	 * Get absolute values and signs. From now on, we can suppose
	 * that x and y fit on 63 bits (we ignore edge conditions).
	 */
	uint64_t sx = x >> 63;
	x = (x ^ -sx) + sx;
	uint64_t sy = y >> 63;
	y = (y ^ -sy) + sy;

	/*
	 * Do a bit by bit division, assuming that the quotient fits.
	 * The numerator starts at x*2^31, and is shifted one bit a time.
	 */
	uint64_t q = 0;
	uint64_t num = x >> 31;
	for (int i = 63; i >= 33; i--) {
		uint64_t b = 1 - ((num - y) >> 63);
		q |= b << i;
		num -= y & -b;
		num <<= 1;
		num |= (x >> (i - 33)) & 1;
	}
	for (int i = 32; i >= 0; i--) {
		uint64_t b = 1 - ((num - y) >> 63);
		q |= b << i;
		num -= y & -b;
		num <<= 1;
	}

	/*
	 * Rounding: if the remainder is at least y/2 (scaled), we add
	 * 2^(-32) to the quotient.
	 */
	uint64_t b = 1 - ((num - y) >> 63);
	q += b;

	/*
	 * Sign management: if the original x and y had different signs,
	 * then we must negate the quotient.
	 */
	sx ^= sy;
	q = (q ^ -sx) + sx;

	return q;
}


int64_t
fxr32_div(int64_t x, int64_t y)
{
	x = inner_fxr32_div(x, y);
	return x;
}

int test_fxr_mul_div()
{
	static double xx[1024], yy[1024], zz [1024];
	uint64_t start, finish, dif;
	uint64_t t_double_min, t_emu_double_min, t_fxr32_min, t_fxr_min;
	uint64_t t_double_max = 0, t_emu_double_max = 0, t_fxr32_max = 0, t_fxr_max = 0;
	int success = 0;
	for (int i = 0; i < 1024; ++i)
	{
		xx[i] = rand()  + 0.12345;
		yy[i] = rand()  + 0.12345;
	}
	
	for (int i = 0; i < 1024; ++i)
	{
		double x = xx[i], y = yy[i], z;
		uint64_t emu_double_x = *(uint64_t*)&x, emu_double_y = *(uint64_t*)&y, emu_double_z;
		uint64_t fxr32_x = (uint64_t)(x * ((uint64_t)1 << 32) + 0.5);
		uint64_t fxr32_y = (uint64_t)(y * ((uint64_t)1 << 32) + 0.5), fxr32_z;
		fxr fxr_x, fxr_y, fxr_z;
		fxr_x.v = (x * ((uint64_t)1 << SCALE) + 0.5);
		fxr_y.v = (y * ((uint64_t)1 << SCALE) + 0.5);
		
		t_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			z = x * y;
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_double_min)
				t_double_min = dif;
		}
		zz[i] = z;
		
		if (t_double_min > t_double_max)
				t_double_max = t_double_min;

		t_emu_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			emu_double_z = emu_double_mul(emu_double_x, emu_double_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_emu_double_min)
				t_emu_double_min = dif;
		}
		if (t_emu_double_min  > t_emu_double_max)
				t_emu_double_max = t_emu_double_min;

		t_fxr32_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr32_z = fxr32_mul(fxr32_x, fxr32_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr32_min)
				t_fxr32_min = dif;
		}
		if (t_fxr32_min > t_fxr32_max)
			t_fxr32_max = t_fxr32_min;
			
		t_fxr_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr_z = fxr_mul(fxr_x, fxr_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr_min)
				t_fxr_min = dif;
		}
		if (t_fxr_min > t_fxr_max)
			t_fxr_max = t_fxr_min;


		success |= z != *(double*)(&emu_double_z);
		success |= (uint64_t)(z + 0.5) != (uint64_t)(fxr32_z >> 32);
		success |= (uint64_t)(z + 0.5) != (uint64_t)(fxr_z.v >> SCALE);
		success = 0;
		if (success != 0)
			printf("");




	}


	printf("mul: t_double_min = %d t_double_max = %d\n", t_double_min, t_double_max);
	printf("mul: t_emu_double_min = %d t_old_emu_double_max = %d\n", t_emu_double_min, t_emu_double_max);
	printf("mul: t_fxr32_min = %d t_fxr32_max = %d\n", t_fxr32_min, t_fxr32_max);
	printf("mul: t_fxr_min = %d t_fxr_max = %d\n", t_fxr_min, t_fxr_max);

	t_double_max = 0, t_emu_double_max = 0, t_fxr32_max = 0, t_fxr_max = 0;
	for (int i = 0; i < 1024; ++i)
	{
		double x = zz[i], y = yy[i], z;
		uint64_t emu_double_x = *(uint64_t*)&x, emu_double_y = *(uint64_t*)&y, emu_double_z;
		uint64_t fxr32_x = (uint64_t)(x * ((uint64_t)1 << 32) + 0.5);
		uint64_t fxr32_y = (uint64_t)(y * ((uint64_t)1 << 32) + 0.5), fxr32_z;
		fxr fxr_x, fxr_y, fxr_z;
		fxr_x.v = (x * ((uint64_t)1 << SCALE) + 0.5);
		fxr_y.v = (y * ((uint64_t)1 << SCALE) + 0.5);

		t_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			z = x / y;
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_double_min)
				t_double_min = dif;
		}

		if (t_double_min > t_double_max)
			t_double_max = t_double_min;

		t_emu_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			emu_double_z = emu_double_div(emu_double_x, emu_double_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_emu_double_min)
				t_emu_double_min = dif;
		}
		if (t_emu_double_min > t_emu_double_max)
			t_emu_double_max = t_emu_double_min;

		t_fxr32_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr32_z = fxr32_div(fxr32_x, fxr32_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr32_min)
				t_fxr32_min = dif;
		}
		if (t_fxr32_min > t_fxr32_max)
			t_fxr32_max = t_fxr32_min;

		t_fxr_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			fxr_z = fxr_div(fxr_x, fxr_y);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr_min)
				t_fxr_min = dif;
		}
		if (t_fxr_min > t_fxr_max)
			t_fxr_max = t_fxr_min;


		success |= z != *(double*)(&emu_double_z);
		success |= (uint64_t)(z * ((uint64_t)1 << 32) + 0.5) != (uint64_t)(fxr32_z);
		success |= (uint64_t)(z * ((uint64_t)1 << SCALE) + 0.5) != (uint64_t)(fxr_z.v);
		if (success != 0)
			printf("");
		success = 0;




	}
	printf("div: t_double_min = %d t_double_max = %d\n", t_double_min, t_double_max);
	printf("div: t_emu_double_min = %d t_old_emu_double_max = %d\n", t_emu_double_min, t_emu_double_max);
	printf("div: t_fxr32_min = %d t_fxr32_max = %d\n", t_fxr32_min, t_fxr32_max);
	printf("div: t_fxr_min = %d t_fxr_max = %d\n", t_fxr_min, t_fxr_max);

	return success;
}

double double_exp(double x)
{
	double d, y;

	d = x;
	y = 0.000000002073772366009083061987;
	y = 0.000000025299506379442070029551 - y * d;
	y = 0.000000275607356160477811864927 - y * d;
	y = 0.000002755586350219122514855659 - y * d;
	y = 0.000024801566833585381209939524 - y * d;
	y = 0.000198412739277311890541063977 - y * d;
	y = 0.001388888894063186997887560103 - y * d;
	y = 0.008333333327800835146903501993 - y * d;
	y = 0.041666666666110491190622155955 - y * d;
	y = 0.166666666666984014666397229121 - y * d;
	y = 0.500000000000019206858326015208 - y * d;
	y = 0.999999999999994892974086724280 - y * d;
	y = 1.000000000000000000000000000000 - y * d;
	return y;
}

static inline uint64_t
emu_double_ursh(uint64_t x, int n)
{
	x ^= (x ^ (x >> 32)) & -(uint64_t)(n >> 5);
	return x >> (n & 31);
}

static inline int64_t
emu_double_sub(int64_t x, int64_t y)
{
	x -= y;
	return x;
}

static inline uint64_t
emu_double_trunc(int64_t x)
{
	uint64_t t, xu;
	int e, cc;

	/*
	 * Extract the absolute value. Since we assume that the value
	 * fits in the -(2^63-1)..+(2^63-1) range, we can left-shift
	 * the absolute value into the 2^62..2^63-1 range, and then
	 * do a right shift afterwards.
	 */
	e = (int)(x >> 52) & 0x7FF;
	xu = ((x << 10) | ((uint64_t)1 << 62)) & (((uint64_t)1 << 63) - 1);
	cc = 1085 - e;
	xu = emu_double_ursh(xu, cc & 63);

	/*
	 * If the exponent is too low (cc > 63), then the shift was wrong
	 * and we must clamp the value to 0. This also covers the case
	 * of an input equal to zero.
	 */
	xu &= -(uint64_t)((uint32_t)(cc - 64) >> 31);

	/*
	 * Apply back the sign, if the source value is negative.
	 */
	t = x >> 63;
	xu = (xu ^ -t) + t;
	return *(int64_t*)&xu;
}

static const uint64_t emu_double_ptwo63 = 4890909195324358656;
uint64_t emu_double_exp(uint64_t x)
{
	uint64_t z, y;
	uint64_t a, b;
	uint32_t z0, z1, y0, y1;
	static const uint64_t C[] = {
		0x00000004741183A3u,
		0x00000036548CFC06u,
		0x0000024FDCBF140Au,
		0x0000171D939DE045u,
		0x0000D00CF58F6F84u,
		0x000680681CF796E3u,
		0x002D82D8305B0FEAu,
		0x011111110E066FD0u,
		0x0555555555070F00u,
		0x155555555581FF00u,
		0x400000000002B400u,
		0x7FFFFFFFFFFF4800u,
		0x8000000000000000u
	};
	y = C[0];
	z = (uint64_t)emu_double_trunc(fxr32_mul (x, emu_double_ptwo63)) << 1;

	for (int u = 1; u < (sizeof C) / sizeof(C[0]); u++) {
		/*
		 * Compute product z * y over 128 bits, but keep only
		 * the top 64 bits.
		 *
		 * TODO: On some architectures/compilers we could use
		 * some intrinsics (__umulh() on MSVC) or other compiler
		 * extensions (unsigned __int128 on GCC / Clang) for
		 * improved speed; however, most 64-bit architectures
		 * also have appropriate IEEE754 floating-point support,
		 * which is better.
		 */
		uint64_t c;

		z0 = (uint32_t)z;
		z1 = (uint32_t)(z >> 32);
		y0 = (uint32_t)y;
		y1 = (uint32_t)(y >> 32);
		a = ((uint64_t)z0 * (uint64_t)y1)
			+ (((uint64_t)z0 * (uint64_t)y0) >> 32);
		b = ((uint64_t)z1 * (uint64_t)y0);
		c = (a >> 32) + (b >> 32);
		c += (((uint64_t)(uint32_t)a + (uint64_t)(uint32_t)b) >> 32);
		c += (uint64_t)z1 * (uint64_t)y1;
		y = C[u] - c;
	}
	return y;
}

int test_fxr_exp()
{
	static double xx[1024], yy[1024];
	uint64_t start, finish, dif;
	uint64_t t_double_min, t_emu_double_min, t_fxr32_min, t_fxr_min;
	uint64_t t_double_max = 0, t_emu_double_max = 0, t_fxr32_max = 0, t_fxr_max = 0;
	int success = 0;
	for (int i = 0; i < 1024; ++i)
	{
		xx[i] = 1. / (rand() + 1);

	}

	for (int i = 0; i < 1024; ++i)
	{
		double x = xx[i], y;
		uint64_t emu_double_x = *(uint64_t*)&x, emu_double_y;

		fxr2 fxr_x, fxr_y;
		fxr_x.v = (x * ((uint64_t)1 << (2 * SCALE)) + 0.5);

		t_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			y = double_exp(x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_double_min)
				t_double_min = dif;
		}
		yy[i] = y;

		if (t_double_min > t_double_max)
			t_double_max = t_double_min;

		t_emu_double_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			emu_double_y = emu_double_exp(x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_emu_double_min)
				t_emu_double_min = dif;
		}
		if (t_emu_double_min > t_emu_double_max)
			t_emu_double_max = t_emu_double_min;



		t_fxr_min = 0xFFFFFFFFFFFFFFFFL;
		for (int j = 0; j < 256; ++j)
		{
			start = __rdtsc();
			// fxr2_exp_
			fxr_y = fxr2_exp_(fxr_x);
			finish = __rdtsc();
			dif = finish - start;
			if (dif < t_fxr_min)
				t_fxr_min = dif;
		}
		if (t_fxr_min > t_fxr_max)
			t_fxr_max = t_fxr_min;


		success |= y != *(double*)(&emu_double_y);

		success |= (uint64_t)(y + 0.5) != (uint64_t)(fxr_y.v >> (2 * SCALE));
		success = 0;
		if (success != 0)
			printf("");




	}
	printf("exp: t_double_min = %d t_double_max = %d\n", t_double_min, t_double_max);
	printf("exp: t_emu_double_min = %d t_old_emu_double_max = %d\n", t_emu_double_min, t_emu_double_max);
	printf("exp: t_fxr_min = %d t_fxr_max = %d\n", t_fxr_min, t_fxr_max);
	return success;
}
